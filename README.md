#Forecasting recycling glass growth in the city of Frankfurt

## Kurze Projekt Beschreibung

### üéØ Business Unterstanding

Die Frankfurter  Entsorgungs- und Service GmbH hat 72 Glascontainer in Frankfurt mit entsprechender Sensorik ausgestattet. Hierbei werden Daten zum F√ºllstand, der Temperatur im Container sowie Neigungswinkel gesammelt.

Das Projekt befindet sich aktuell noch in einer Explorationsphase was in Folge bedeutet, dass es noch keinen standardisierten Ansatz gibt diese zu verwerten.

Mithilfe der Daten soll ein effizienteres leeren der Container erm√∂glicht werden. Es gilt somit ein unterst√ºtzendes Prediktionsmodell zu entwickeln, welches Aussagen √ºber das F√ºllverhalten der Container treffen kann.

## üìä Data Understanding

Bei den Daten handelt es sich um Zeitreihen, die aus der Beobachtung einzelner Variablen √ºber einen Zeitraum entstehen. Die entscheide Variable **H√∂he** wird dabei unterschiedlich oft beobachten allerdings im Schnitt mehrmals pro Tag. Da es sich dabei allerdings um EDGE Ger√§te handelt, sind einzelne Messungen nicht immer akkurat und zus√§tzlich sind h√§ufig fehlende Daten zu beobachten. So k√∂nnen die Messungen f√ºr unterschiedliche Container stark varieren. Im folgenden Bild ist die F√ºllh√∂he eines Containers √ºber den bereitgestellten Zeitraum zu beobachten.

![Untitled%20dc564a8fe174488583651b8e230af1de/brauchbar.png](doku_ressources/brauchbar.png)

[DataVisualisation.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/DataVisualization.ipynb)

Um Einfl√ºsse auf den F√ºllstand besser zu verstehen, haben wir uns unterschiedliche Datenquellen aus dem Internet besorgt und diese mithilfe einer Korrelationsanalyse auf ihren Einfluss √ºberpr√ºft.

Untersuchte Datenquellen:

‚òÅÔ∏è  Wetterdaten - Intuition: Tendenziell wird Glas eher bei gutem Wetter entsorgt als bei Regen. Wer verl√§sst schon gerne bei Regen das Haus?

üéâ  Feiertage - Intuition: √úber Feiertage kommen Menschen gerne zusammen, was zu einem vermehrten aufkommen Restglas f√ºhren k√∂nnte (Weinflaschen etc.)

‚öΩ  Fu√üballspiele & Gro√üveranstaltungen - Intuition: Zu Fu√üballspielen und anderen Gro√üveranstaltungen ist das Menschenaufkommen in der Stadt erh√∂ht. Auch hier wird wieder vermehrt Alkohol konsumiert und evtl. direkt in auf dem Weg liegende Glascontainern entsorgt.

[webscraper.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/webscraper.ipynb)

## üìà  Data Preparation

Da Qualit√§t der Vorhersagen des Modells direkt mit der Qualit√§t der Trainingsdaten zusammenh√§ngt, galt es die Daten m√∂glichst gut f√ºr das Training vorzubereiten.

Nachdem alle Containerf√ºllst√§nde visualisiert wurden, wurden Container, deren Datenqualit√§t extrem fragw√ºrdig waren √ºber eine deskriptive Analyse aussortiert. In den folgenden Plots sind jeweils ein Beispiel f√ºr einen Aussortierten (unten) und einen Weiterverwendeten (oben) zu sehen.

![Untitled%20dc564a8fe174488583651b8e230af1de/brauchbar%201.png](doku_ressources/brauchbar%201.png)

![Untitled%20dc564a8fe174488583651b8e230af1de/unbrauchbar.png](doku_ressources/unbrauchbar.png)

In den weiteren Schritten wurden offensichtliche Messfehler in den Container entfernt z. B. H√∂he > 190. Da in den Daten weiterhin sehr viel Noise enthalten ist galt es, denn unterliegenden Trend zu extrahieren. Hierbei wurden unterschiedliche Smoothing Verfahren angewandt. Im Code Snippet sowie in der Grafik unten wurde jeweils das Smoothing Verfahren des Rolling Average mit einer Fenstergr√∂√üe von 30 visualisiert.

```python
df['mov_avg'] = df['Height'].rolling(30).mean()
```

![Untitled%20dc564a8fe174488583651b8e230af1de/plot.png](doku_ressources/plot.png)

Au√üerdem haben wir die Daten, nach dem erste Modellierungsversuche scheiterten, auf Tagesniveau mithilfe mean(), min() und max() Operators aggregiert. Dabei waren die Ergebnisse des mean() n√§her am Rohverlauf als die anderen, weshalb f√ºr das weitere Vorgehen diese Daten verwendet wurden. 
Da bei den meisten Containern nicht f√ºr jeden Tag Messwerte existierten, haben wir fehlende Messwerte mithilfe von Interpolationsverfahren aufgef√ºllt.

### üåÄ Clustering

Da sich bei der deskriptiven Analyse zeigte, dass es visuelle √Ñhnlichkeiten zwischen den Zeitreihen gibt, wollte wir diese zu Clustern zusammenfassen und f√ºr jedes Cluster ein Modell trainieren um so die G√ºte der einzelnen und damit die gesamt Vorhersagequalit√§t zu erh√∂hen. 

Daf√ºr wurde Dynamic Time Warping in Kombination mit K-means Clustering eingesetzt. Daf√ºr wurde die Bibliothek [tslearn](https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html) verwendet. Dabei hat sich **k = 3** als effektivste Anzahl f√ºr die Cluster herauskristallisiert.

![Untitled%20dc564a8fe174488583651b8e230af1de/clusters.png](doku_ressources/clusters.png)

![Untitled%20dc564a8fe174488583651b8e230af1de/Screenshot_2021-07-24_at_04.05.14.png](doku_ressources/Screenshot_2021-07-24_at_04.05.14.png)

[preprocessing_clustering.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/preprocessing_clustering.ipynb)

[data_preprocessing.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/data_preprocessing.ipynb)

[mapsVizualization.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/mapsVizualization.ipynb)

## ü§ñ Modelling

### Lineare Regression als Baseline

‚ùó Die wesentliche Problem f√ºr das Modellieren sind die eher schlechte Datenqualit√§t sowie die wechselnden Standorte der Container.

Dabei macht es keinen Sinn die eigentlichen Kurven mit ihren jeweiligen Leerungsintervallen zu leeren, da ja gerade diese angepasst werden sollen.

Als Baseline haben wir versucht dies mithilfe einer einfachen linearen Regression zu modellieren. Dabei wurde jeweils aus einer Zeitreihe pro Cluster ein F√ºllintervall extrahiert und darauf die Steigung berechnet. Dies f√ºhrte f√ºr diese recht einfache Methode auch zu sehr vielversprechenden Ergebnissen. Um die Ergebnisse zu evaluieren, haben wir versucht, die jeweiligen Leerungen mithilfe einer Schwellwertfunktion zu berechnen. Dies funktioniert auf unterschiedlichen Zeitreihen mit stark schwankendem Erfolg.

![Untitled%20dc564a8fe174488583651b8e230af1de/Screenshot_2021-07-24_at_17.14.01.png](doku_ressources/Screenshot_2021-07-24_at_17.14.01.png)

![Untitled%20dc564a8fe174488583651b8e230af1de/Untitled.png](doku_ressources/Untitled.png)

Das Verfahren sieht auf den ersten Blick tats√§chlich sehr vielsprechend aus, kann aber nicht optimal evaluiert werden, da in den Daten die Zeitpunkte der Leerung garnicht, bis sehr schlecht angegeben sind.

**Hierf√ºr w√§ren Daten notwendig, die die korrekten Leerungszeitpunkte enthalten.** 

[linear_regression.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/performing_linear_regression.ipynb)

[linearRegression.ipynb](https://git.scc.kit.edu/uflgi/bda-analytics-challenge-template/-/blob/master/notebooks/linearRegression.ipynb)

### LSTM

Drei Ans√§tze f√ºr ein LSTM wurden verfolgt: 
    1.   LSTM auf Basis eines Clusters mit Daten f√ºr jede Stunde (Notebook: Cluster0_LSTM.ipynb, Modell: 
    2.   LSTM auf Basis aller Cluster mit Daten f√ºr jede Stunde (Notebook: Overall_LSTM.ipynb, Modell: 
    3.   LSTM auf Basis aller Cluster mit Daten f√ºr jeden Tag (Notebook: Overall_grouped_LSTM.ipynb, Modell: 
    
Nur Ansatz 3. erzeugt passende Vorhersagen: 

![Untitled%20dc564a8fe174488583651b8e230af1de/Overall_pred.png](doku_ressources/Overall_pred.png)

Hierbei lernt das LSTM das F√ºllverhalten aller Container. Die Leerungen nehmen wir manuell vor, indem die Input Werte f√ºr das LSTM auf einen leeren Container zur√ºckgesetzt werden, sobald der F√ºllstand bei mehreren Vorhersagen gleich bleibt.

Dieser Ansatz w√§re mit zus√§tzlichen F√ºllstandsdaten auch f√ºr einzelne Container m√∂glich.

## üöÄ Fazit/Future Work

Die Lineare Regression trifft die Entwicklung des F√ºllstands in vielen F√§llen bereits sehr gut, was daf√ºr spricht, dass der F√ºllstand keiner komplexen Funktion folgt und daher in der Tendenz ein relativ einfach zu modellierender Trend zugrunde liegt.

‚ùó Hierbei wurden allerdings keine Ausrei√üer F√ºllst√§nde mit in Betracht gezogen. Um diese zu modellieren ist die Kapazit√§t einer einfachen Regression nicht hoch genug. 

Der Engpass f√ºr die Evaluation ist hier klar der korrekte Zeitpunkt der Leerung. Mit unserer einfachen Schwellwertfunktion sind wir nicht sicher in der Lage diese zu genau genug zu approximieren.

‚Üí Zwar sagt die Sensor Dokumentation, dass diese enthalten seien sollten, dies konnten wir allerdings in den Daten nicht beobachten. Hierf√ºr kann entweder der Sensor adjustiert werden oder um aus diesen Daten mehr zu machen, w√§re unser Vorschlag einen Teil der Daten manuell zu Labeln und dann √ºber ein Supervised Learning Verfahren ein Modell zu trainieren, dass in der Lage ist die Leerungszeitpunkte genauer anzugeben. Leider kam uns diese Idee zu sp√§t und konnte im Rahmen dieses Projekts leider nicht mehr umgesetzt werden.



## Special Stuff
Our LSTM model was trained on Google Colab. The data split and model training is not performed in the notebooks, but 
the processed is visualized. The whole functionality can be found in the src folder.


## Project Organization
------------
```
	‚îú‚îÄ‚îÄ README.md 							<-- this file. insert group members here
	‚îú‚îÄ‚îÄ .gitignore 						    <-- prevents you from submitting several clutter files
	‚îú‚îÄ‚îÄ data
	‚îÇ¬†¬† ‚îú‚îÄ‚îÄ modeling
	‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dev 						<-- your development set goes here
	‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test 						<-- your test set goes here
	‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train 						<-- your train set goes here goes here
	‚îÇ¬†¬† ‚îú‚îÄ‚îÄ preprocessed 					<-- your preprocessed data goes here
	‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw								<-- the provided raw data for modeling goes here
	‚îú‚îÄ‚îÄ docs								<-- provided explanation of raw input data goes here
	‚îÇ
	‚îú‚îÄ‚îÄ models								<-- dump models here
	‚îú‚îÄ‚îÄ notebooks							<-- your playground for juptyer notebooks
	‚îú‚îÄ‚îÄ requirements.txt 					<-- required packages to run your submission (use a virtualenv!)
	‚îî‚îÄ‚îÄ src
	 ¬†¬† ‚îú‚îÄ‚îÄ additional_features.py 			<-- your creation of additional features/data goes here
	 ¬†¬† ‚îú‚îÄ‚îÄ cluster.py 						<-- code for clustering
	    ‚îú‚îÄ‚îÄ regression.py 					<-- regression model
	    ‚îú‚îÄ‚îÄ preprocessing.py 				<-- your preprocessing script goes here
	    ‚îú‚îÄ‚îÄ train.py 						<-- your training script goes here
	    ‚îî‚îÄ‚îÄ predict.py 						<-- prediction script for LSTM
	
```

